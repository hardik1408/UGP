{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1)  # 512 -> 256\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)  # 256 -> 128\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)  # 128 -> 64\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)  # 64 -> 32\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 1024)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(1024, 256)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))  # 512 -> 256\n",
    "        x = torch.tanh(self.conv2(x))  # 256 -> 128\n",
    "        x = torch.tanh(self.conv3(x))  # 128 -> 64\n",
    "        x = torch.tanh(self.conv4(x))  # 64 -> 32\n",
    "        x = self.flatten(x)\n",
    "        x = torch.tanh(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.tanh(self.fc2(x))  # Fully connected layer\n",
    "        return x  # Final compressed representation\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(256, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128 * 32 * 32)\n",
    "        self.unflatten = nn.Unflatten(1, (128, 32, 32))\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)  # 32 -> 64\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)  # 64 -> 128\n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1)  # 128 -> 256\n",
    "        self.convt4 = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=3, stride=2, padding=1, output_padding=1)  # 256 -> 512\n",
    "\n",
    "        # Additional outputs for intermediate resolutions\n",
    "        self.out_128 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1)  # Output at 128x128\n",
    "        self.out_256 = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=1)  # Output at 256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))  # Fully connected layer\n",
    "        x = torch.tanh(self.fc2(x))  # Fully connected layer\n",
    "        x = self.unflatten(x)\n",
    "        x = torch.tanh(self.convt1(x))  # 32 -> 64\n",
    "        x = torch.tanh(self.convt2(x))  # 64 -> 128\n",
    "\n",
    "        out_128 = torch.tanh(self.out_128(x))  # 128x128 output\n",
    "\n",
    "        x = torch.tanh(self.convt3(x))  # 128 -> 256\n",
    "        out_256 = torch.tanh(self.out_256(x))  # 256x256 output\n",
    "\n",
    "        x = torch.tanh(self.convt4(x))  # 256 -> 512\n",
    "        out_512 = torch.tanh(x)  # 512x512 output\n",
    "\n",
    "        return out_128, out_256, out_512\n",
    "\n",
    "\n",
    "class AutoEncoderMRL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoderMRL, self).__init__()\n",
    "        self.encoder = Encoder()  # Downsampling: 512 -> 32\n",
    "        self.decoder = Decoder()  # Upsampling: 32 -> 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.encoder(x)  # Get compressed representation\n",
    "        out_128, out_256, out_512 = self.decoder(emb)  # Reconstruct at multiple resolutions\n",
    "        return out_128, out_256, out_512, emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def mrl_loss(outputs, target_128, target_256, target_512, weights):\n",
    "    \"\"\"\n",
    "    Compute the MRL loss as a weighted sum of PSNR losses for 128x128, 256x256, and 512x512 resolutions.\n",
    "\n",
    "    :param out_128: Reconstructed output at 128x128.\n",
    "    :param out_256: Reconstructed output at 256x256.\n",
    "    :param out_512: Reconstructed output at 512x512.\n",
    "    :param target_128: Ground truth image at 128x128.\n",
    "    :param target_256: Ground truth image at 256x256.\n",
    "    :param target_512: Ground truth image at 512x512.\n",
    "    :param weights: List of weights for each resolution [w_128, w_256, w_512].\n",
    "    :return: Combined loss value.\n",
    "    \"\"\"\n",
    "    # Compute PSNR for each resolution\n",
    "    out_128, out_256, out_512 = outputs\n",
    "    loss_128 = -psnr(out_128, target_128, data_range=1.0)\n",
    "    loss_256 = -psnr(out_256, target_256, data_range=1.0)\n",
    "    loss_512 = -psnr(out_512, target_512, data_range=1.0)\n",
    "\n",
    "    print(f\"PSNR Loss at 128x128: {-loss_128:.4f}, 256x256: {-loss_256:.4f}, 512x512: {-loss_512:.4f}\")\n",
    "\n",
    "    # Weighted sum of losses\n",
    "    total_loss = weights[0] * loss_128 + weights[1] * loss_256 + weights[2] * loss_512\n",
    "    return total_loss / 3  # Average the weighted loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset for loading images from a single folder\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        # Sort image files to ensure consistent order\n",
    "        self.image_files = sorted([f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.folder_path, self.image_files[idx])\n",
    "        image = Image.open(img_path)  # Open the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # Return the transformed image\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Folder paths for different resolutions\n",
    "folder_path_512 = 'data/train/512'\n",
    "folder_path_256 = 'data/train/256'\n",
    "folder_path_128 = 'data/train/128'\n",
    "\n",
    "# Create datasets for each resolution\n",
    "dataset_512 = ImageDataset(folder_path_512, transform=transform)\n",
    "dataset_256 = ImageDataset(folder_path_256, transform=transform)\n",
    "dataset_128 = ImageDataset(folder_path_128, transform=transform)\n",
    "\n",
    "# Define dataloaders\n",
    "batch_size = 32\n",
    "dataloader_512 = DataLoader(dataset_512, batch_size=batch_size, shuffle=False)\n",
    "dataloader_256 = DataLoader(dataset_256, batch_size=batch_size, shuffle=False)\n",
    "dataloader_128 = DataLoader(dataset_128, batch_size=batch_size, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
